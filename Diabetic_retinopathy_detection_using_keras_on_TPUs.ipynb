{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabetic retinopathy detection using keras on TPUs",
      "provenance": [],
      "authorship_tag": "ABX9TyNH8xB/ivd1HFBMdyB5HM4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshShah03325/Healthcare-examles-using-Keras/blob/main/Diabetic_retinopathy_detection_using_keras_on_TPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "B1Hi76jb-j7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "metadata": {
        "id": "SsIQprEd4GaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir diabtetic_retinopathy\n",
        "!gcsfuse diabetic_dataset/retinopathy/ diabtetic_retinopathy"
      ],
      "metadata": {
        "id": "Y8ea01I0G-ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "# import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# now part of tensorflow\n",
        "import efficientnet.tfkeras as efn\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re, math\n",
        "import time"
      ],
      "metadata": {
        "id": "r8fZNsDe3GaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "U6swyC--3MoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"TPU\"\n",
        "\n",
        "# TFRecords file for training (not Kaggle location !)\n",
        "TFREC_DIR = './diabetic_retinopathy'\n",
        "\n",
        "# number of folds for CV\n",
        "FOLDS = 5\n",
        "\n",
        "# WHICH IMAGE SIZES TO LOAD \n",
        "# CHOOSE 128, 192, 256, 384, 512, 768 \n",
        "\n",
        "IMG_SIZES = 512\n",
        "\n",
        "IMAGE_SIZE = [IMG_SIZES, IMG_SIZES]\n",
        "\n",
        "\n",
        "# tune it, dependes on Image, size, TPU or GPU\n",
        "BATCH_SIZE = 12\n",
        "\n",
        "EPOCHS = 25\n",
        "\n",
        "# WHICH EFFICIENTNET TO USE (B?, B0 from B7)\n",
        "EFF_NETS = 4\n",
        "\n",
        "# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\n",
        "WGTS = 1/FOLDS\n",
        "\n",
        "NUM_CLASSES = 5"
      ],
      "metadata": {
        "id": "eNYGqVWVAa_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        print(\"Could not connect to TPU\")\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except _:\n",
        "            print(\"failed to initialize TPU\")\n",
        "    else:\n",
        "        DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE == \"GPU\":\n",
        "    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n",
        "    print(\"Num GPUs Available: \", n_gpu)\n",
        "    \n",
        "    if n_gpu > 1:\n",
        "        print(\"Using strategy for multiple GPU\")\n",
        "        strategy = tf.distribute.MirroredStrategy()\n",
        "    else:\n",
        "        print('Standard strategy for GPU...')\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync\n",
        "\n",
        "print(f'REPLICAS: {REPLICAS}')\n"
      ],
      "metadata": {
        "id": "7ohjvCsNAbDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "  # introduced in TF 2.3\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.4),\n",
        "])"
      ],
      "metadata": {
        "id": "c0V9KCxXAbHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        'patient_id' : tf.io.FixedLenFeature([], tf.int64), \n",
        "        'side' : tf.io.FixedLenFeature([], tf.int64),\n",
        "        'label' : tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    \n",
        "    image = decode_image(example['image'])\n",
        "    patient_id = example['patient_id']\n",
        "    side = example['side']\n",
        "    label = example['label']\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    \n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
        "    return image\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n",
        "         for filename in filenames]\n",
        "    return np.sum(n)"
      ],
      "metadata": {
        "id": "iL672c6TAbLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(filenames, labeled=True, ordered=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
        "    dataset = dataset.map(read_labeled_tfrecord)\n",
        "    # returns a dataset of (image, labels) pairs if labeled=True or (image, id) pairs if labeled=False\n",
        "    return dataset\n",
        "\n",
        "def get_training_dataset(filenames):\n",
        "    dataset = load_dataset(filenames, labeled=True)\n",
        "    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def get_test_dataset(filenames):\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.map(read_unlabeled_tfrecord)    \n",
        "    dataset = dataset.batch(BATCH_SIZE*REPLICAS)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "tB_wAxziBXdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n",
        "        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n",
        "\n",
        "# as default it used B0\n",
        "\n",
        "def build_model(dim = 256, ef = 0):\n",
        "    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n",
        "    \n",
        "    # introdotta la data augmentation come parte del modello\n",
        "    x = data_augmentation(inp)\n",
        "    \n",
        "    base = EFNS[ef](input_shape=(*IMAGE_SIZE, 3), weights='imagenet', include_top = False)\n",
        "    \n",
        "    x = base(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs = inp,outputs = x)\n",
        "    \n",
        "    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
        "    # loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0) \n",
        "    \n",
        "    model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "qQmQrGEhBXg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_test = build_model(ef=3)\n",
        "\n",
        "model_test.summary()"
      ],
      "metadata": {
        "id": "9Os_g9daBXlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr_callback(batch_size=8):\n",
        "    lr_start   = 0.000005\n",
        "    lr_max     = 0.000020 * REPLICAS * batch_size/16\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 2\n",
        "    lr_decay   = 0.8\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
        "            \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
        "            \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
        "    \n",
        "    return lr_callback"
      ],
      "metadata": {
        "id": "l_UA4cCMBXqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHOW_FILES = True\n",
        "VERBOSE = 1\n",
        "PLOT = 1\n",
        "\n",
        "skf = KFold(n_splits = FOLDS, shuffle = True, random_state=42)\n",
        "\n",
        "# for others investigations\n",
        "# we store all the history\n",
        "histories = []\n",
        "\n",
        "# these will be split in folds\n",
        "num_total_train_files = len(tf.io.gfile.glob(TFREC_DIR + '/train*.tfrec'))\n",
        "print(num_total_train_files)\n",
        "\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(np.arange(num_total_train_files))):\n",
        "    \n",
        "    tStart = time.time()\n",
        "    \n",
        "    # display fold info\n",
        "    print('#'*60) \n",
        "    print('#### FOLD', fold+1)\n",
        "    \n",
        "    print('#### Image Size %i, EfficientNet B%i, batch_size %i'%\n",
        "          (IMG_SIZES, EFF_NETS, BATCH_SIZE*REPLICAS))\n",
        "    print('#### Epochs: %i' %(EPOCHS))\n",
        "    \n",
        "    # CREATE TRAIN AND VALIDATION SUBSETS\n",
        "    files_train = tf.io.gfile.glob([TFREC_DIR + '/train%.2i*.tfrec'%x for x in idxT])\n",
        "    \n",
        "    np.random.shuffle(files_train) \n",
        "    print('#'*60)\n",
        "    \n",
        "    files_valid = tf.io.gfile.glob([TFREC_DIR + '/train%.2i*.tfrec'%x for x in idxV])\n",
        "    \n",
        "    if SHOW_FILES:\n",
        "        print('Number of training images', count_data_items(files_train))\n",
        "        print('Number of validation images', count_data_items(files_valid))\n",
        "        \n",
        "    # BUILD MODEL\n",
        "    if DEVICE=='TPU':\n",
        "        # to avoid OOM\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            \n",
        "    K.clear_session()\n",
        "    with strategy.scope():\n",
        "        model = build_model(dim=IMG_SIZES, ef=EFF_NETS)\n",
        "        \n",
        "    # callback to save best model for each fold\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        'fold-%i.h5'%fold, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    \n",
        "    csv_logger = tf.keras.callbacks.CSVLogger('training_retina-%i.log'%fold)\n",
        "\n",
        "    history = model.fit(\n",
        "        get_training_dataset(files_train), \n",
        "        epochs=EPOCHS, \n",
        "        callbacks = [sv, get_lr_callback(BATCH_SIZE), csv_logger], \n",
        "        steps_per_epoch = count_data_items(files_train)/BATCH_SIZE//REPLICAS,\n",
        "        validation_data = get_training_dataset(files_valid), \n",
        "        validation_steps = count_data_items(files_valid)/BATCH_SIZE//REPLICAS,\n",
        "        verbose=VERBOSE)\n",
        "    \n",
        "    # save all histories\n",
        "    histories.append(history)\n",
        "    \n",
        "    tElapsed = round(time.time() - tStart, 1)\n",
        "    \n",
        "    print(' ')\n",
        "    print('Time (sec) elapsed for fold: ', tElapsed)\n",
        "    print('...')\n",
        "    print('...')\n"
      ],
      "metadata": {
        "id": "ObA6XN57AbP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the training loss\n",
        "def plot_loss(hist):\n",
        "    plt.figure(figsize=(14,6))\n",
        "    \n",
        "    plt.plot(hist.history['loss'], label='Training loss')\n",
        "    plt.plot(hist.history['val_loss'], label='Validation loss')\n",
        "    plt.title('Loss fold n. ' + str(fold + 1) )\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.grid()\n",
        "    plt.show();"
      ],
      "metadata": {
        "id": "WGJu54wmAysb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the training loss for all 5 folds\n",
        "for fold in range(FOLDS):\n",
        "    plot_loss(histories[fold]"
      ],
      "metadata": {
        "id": "MjvV7Ii4AzAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLot the training accuracy\n",
        "def plot_acc(hist):\n",
        "    plt.figure(figsize=(14,6))\n",
        "    \n",
        "    plt.plot(hist.history['accuracy'], label='Training accuracy')\n",
        "    plt.plot(hist.history['val_accuracy'], label='Validation accuracy')\n",
        "    plt.title('Accuracy fold n. ' + str(fold + 1) )\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.ylabel('Acc')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.grid()\n",
        "    plt.show();"
      ],
      "metadata": {
        "id": "NFUncALQAzEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PLot accuracy for all 5 folds\n",
        "for fold in range(FOLDS):\n",
        "    plot_acc(histories[fold])"
      ],
      "metadata": {
        "id": "U6gMxkOqAzJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the test accuracy\n",
        "files_test = tf.io.gfile.glob(TFREC_DIR + '/test*.tfrec')\n",
        "num_total_test_files = len(tf.io.gfile.glob(TFREC_DIR + '/test*.tfrec'))\n",
        "wi = [1/FOLDS]*FOLDS\n",
        "avg_acc = 0\n",
        "\n",
        "for fold in range(FOLDS):\n",
        "    model.load_weights('fold-%i.h5'%fold)\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(get_test_dataset(files_test), verbose = 0, batch_size = 4*BATCH_SIZE,\n",
        "                                        steps = num_total_test_files/4*BATCH_SIZE//REPLICAS)\n",
        "\n",
        "    print('Test accuracy fold n.', fold+1, ': ', round(train_acc, 4))\n",
        "\n",
        "    test_avg_acc += train_acc * wi[fold]\n",
        "\n",
        "print('Average test accuracy: ', round(train_avg_acc,4))"
      ],
      "metadata": {
        "id": "NIr77YBWAzPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}